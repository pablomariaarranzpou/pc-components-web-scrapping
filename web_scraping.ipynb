{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e8a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "from urllib.request import urlopen\n",
    "import smtplib\n",
    "from flask import Flask, request, Response\n",
    "import re\n",
    "import time \n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ec3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    html = urlopen(url)\n",
    "    return BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c0bca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar los datos de los procesadores a un csv\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "url = \"https://www.pc-kombo.com/us/components/cpus\"\n",
    "soup = make_soup(url)\n",
    "\n",
    "procesadores = []\n",
    "for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "    if(i != 'None' and i != -1):\n",
    "        nombre = i.find(\"h5\", {'class': 'name'})\n",
    "        subtitle = i.find(\"div\", {'class': 'subtitle'})\n",
    "        turbo = ''\n",
    "        clock = ''\n",
    "        threads = ''\n",
    "        cores = ''\n",
    "        price = ''\n",
    "        socket = None  # Inicializar la variable socket\n",
    "        # buscar el valor del socket\n",
    "        socket_html = subtitle.find(\"span\", {'class': 'socket'})\n",
    "        if socket_html is not None:\n",
    "            socket = socket_html.text.strip()\n",
    "        for atributo in subtitle.find_all('span'):\n",
    "            if atributo.text.startswith('Turbo'):\n",
    "                turbo = atributo.text.split(' ')[1]\n",
    "            elif atributo.text.startswith('Clock'):\n",
    "                clock = atributo.text.split(' ')[1]\n",
    "            elif atributo.text.endswith('Threads'):\n",
    "                threads = atributo.text.split(' ')[0]\n",
    "            elif atributo.text.endswith('Cores'):\n",
    "                cores = atributo.text.split(' ')[0]\n",
    "        # buscar el precio en el HTML\n",
    "        precio_html = i.find(\"span\", {'class': 'price'})\n",
    "        if precio_html is not None:\n",
    "            # obtener el valor del precio y asignarlo a la variable price\n",
    "            price = precio_html.text.strip()[4:]\n",
    "    \n",
    "        if price !='':\n",
    "            url_cpu = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "            response = requests.get(url_cpu)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Obtener el performance level\n",
    "            perf_level = soup.find('p', {'itemprop': 'aggregateRating'})\n",
    "            if perf_level is not None:\n",
    "                perf_level = perf_level.text\n",
    "                perf_level_pattern = re.compile(r'(\\d+\\.\\d+/\\d+)')\n",
    "                perf_level_match = perf_level_pattern.search(perf_level)\n",
    "\n",
    "                if perf_level_match is not None:\n",
    "                    perf_level = perf_level_match.group(1)\n",
    "                    perf_level = perf_level[:-3]\n",
    "                    \n",
    "            # Obtener el productor\n",
    "            producer = soup.find(\"dd\",{'itemprop':'brand'} )\n",
    "            if producer is not None:\n",
    "                    producer = producer.text\n",
    "            \n",
    "            procesador = {\n",
    "            'nombre': nombre.get_text(strip=True),\n",
    "            'socket': socket,\n",
    "            'turbo': turbo,\n",
    "            'clock': clock,\n",
    "            'threads': threads,\n",
    "            'cores': cores,\n",
    "            'perf_level' : perf_level,\n",
    "            'producer': producer,\n",
    "            'precio': price\n",
    "            }\n",
    "            \n",
    "            procesadores.append(procesador)\n",
    "\n",
    "# Escribir los datos en un archivo CSV\n",
    "with open('procesadores.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Escribir la cabecera del archivo CSV\n",
    "    writer.writerow(['Nombre', 'Socket', 'Turbo', 'Clock', 'Threads', 'Cores','Performance Level /10','Producer', 'Precio'])\n",
    "    # Escribir cada fila en el archivo CSV\n",
    "    for procesador in procesadores:\n",
    "        writer.writerow([procesador['nombre'], procesador['socket'], procesador['turbo'], \n",
    "                         procesador['clock'], procesador['threads'], procesador['cores'], \n",
    "                         procesador['perf_level'],procesador['producer'], procesador['precio']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afe7025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â exporta los datos necesarios de las gpus\n",
    "import csv\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "url = \"https://www.pc-kombo.com/us/components/gpus\"\n",
    "soup = make_soup(url)\n",
    "\n",
    "gpus = []\n",
    "for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "    if(i != 'None' and i != -1):\n",
    "        nombre = i.find(\"h5\", {'class': 'name'})\n",
    "        subtitle = i.find(\"div\", {'class': 'subtitle'}).find_all('span')\n",
    "        serie = ''\n",
    "        vram = ''\n",
    "        potencia = ''\n",
    "        serie =i.find(\"span\", {'class': 'series'}).text\n",
    "        vram = i.find(\"span\", {'class': 'vram'}).text[:-2]\n",
    "        for atributo in subtitle:\n",
    "            if atributo.text.endswith('W'):\n",
    "                potencia = atributo.text[:-1]\n",
    "                \n",
    "        # buscar el precio en el HTML\n",
    "        precio_html = i.find(\"span\", {'class': 'price'})\n",
    "        if precio_html is not None:\n",
    "            # obtener el valor del precio y asignarlo a la variable price\n",
    "            price = precio_html.text.strip()[4:]  \n",
    "            \n",
    "        if price != '':\n",
    "            url_ram = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "            response = requests.get(url_ram)\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Obtener el productor\n",
    "            producer = soup.find(\"dd\",{'itemprop':'brand'} )\n",
    "            if producer is not None:\n",
    "                producer = producer.text\n",
    "            \n",
    "\n",
    "            #obtener el length\n",
    "            length = soup.find('dt', text='Length')\n",
    "            if length is not None:\n",
    "                length = length.find_next_sibling('dd').text[:-2]\n",
    "\n",
    "            #obtener los slots\n",
    "            slots = soup.find('dt', text='Slots')\n",
    "            if slots is not None:\n",
    "                slots = slots.find_next_sibling('dd').text \n",
    "            \n",
    "            #obtener los 8 pin\n",
    "            eight_pin = soup.find('dt', text='8-pin connectors')\n",
    "            if eight_pin is not None:\n",
    "                eight_pin = eight_pin.find_next_sibling('dd').text\n",
    "            else:\n",
    "                eight_pin = 0\n",
    "\n",
    "            #obtener los 6 pin\n",
    "            six_pin = soup.find('dt', text='6-pin connectors')\n",
    "            if six_pin is not None:\n",
    "                six_pin = six_pin.find_next_sibling('dd').text\n",
    "            \n",
    "            #obtener el boost clock\n",
    "            boost_clock = soup.find('dt', text='Boost Clock')\n",
    "            if boost_clock is not None:\n",
    "                boost_clock = boost_clock.find_next_sibling('dd').text[:-3]\n",
    "                \n",
    "            #obtener memory clock\n",
    "            memory_clock = soup.find('dt', text='Memory Clock')\n",
    "            if memory_clock is not None:\n",
    "                memory_clock = memory_clock.find_next_sibling('dd').text[:-3]\n",
    "                \n",
    "            # Obtener el performance level\n",
    "            perf_level = soup.find('meta', {'itemprop': 'ratingValue'})\n",
    "            if perf_level is not None:\n",
    "                perf_level = perf_level.text\n",
    "                perf_level_pattern = re.compile(r'(\\d+\\.\\d+/\\d+)')\n",
    "                perf_level_match = perf_level_pattern.search(perf_level)\n",
    "\n",
    "                if perf_level_match is not None:\n",
    "                    perf_level = perf_level_match.group(1)\n",
    "                    perf_level = perf_level[:-3]\n",
    "\n",
    "                    \n",
    "            gpu = {\n",
    "            'nombre': nombre.get_text(strip=True),\n",
    "            'serie': serie,\n",
    "            'vram': vram,\n",
    "            'potencia': potencia,\n",
    "            'length':length,\n",
    "            'slots': slots,\n",
    "            'eight_pin': eight_pin,\n",
    "            'six_pin' : six_pin,\n",
    "            'boost_clock': boost_clock,\n",
    "            'memory_clock': memory_clock,\n",
    "            'producer': producer,\n",
    "            'perf_level': perf_level,\n",
    "            'precio': price\n",
    "            }\n",
    "            gpus.append(gpu)\n",
    "\n",
    "# Escribir los datos en un archivo CSV\n",
    "with open('gpus.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Escribir la cabecera del archivo CSV\n",
    "    writer.writerow(['Name', 'Serie', 'VRAM', 'TDP (W)', 'Length (mm)', 'slots', '8 Pin', '6 Pin', 'Boost Clock','Memory Clock', 'Producer', 'Performance Level /10','Price'])\n",
    "    # Escribir cada fila en el archivo CSV\n",
    "    for gpu in gpus:\n",
    "        writer.writerow([gpu['nombre'], gpu['serie'], gpu['vram'], \n",
    "                         gpu['potencia'], gpu['length'], gpu['slots'], gpu['eight_pin'], gpu['six_pin'],\n",
    "                         gpu['boost_clock'], gpu['memory_clock'], gpu['producer'], gpu['perf_level'], gpu['precio']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac264f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67fd9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las motherboards \n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_motherboards():\n",
    "    url = \"https://www.pc-kombo.com/us/components/motherboards\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    motherboards = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "            size = i.find(\"span\", {'class': 'size'}).text\n",
    "            socket = i.find(\"span\", {'class': 'socket'}).text\n",
    "            chipset = i.find(\"span\", {'class': 'chipset'}).text\n",
    "            ramslots = i.find(\"span\", {'class': 'ramslots'}).text\n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_motherboard = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_motherboard)\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                \"\"\" # Obtener el form factor\n",
    "                form_factor = soup.find('dt', text='Form Factor')\n",
    "                if form_factor is not None:\n",
    "                    form_factor = form_factor.find_next_sibling('dd').text\"\"\"\n",
    "\n",
    "                # Obtener el tipo de memoria y su capacidad\n",
    "                memory_type = soup.find('dt', text='Memory Type')\n",
    "                if memory_type is not None:\n",
    "                    memory_type = memory_type.find_next_sibling('dd').text\n",
    "                    \n",
    "                memory_capacity = soup.find('dt', text='Memory Capacity')\n",
    "                if memory_capacity is not None:\n",
    "                    memory_capacity = memory_capacity.find_next_sibling('dd').text  \n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                motherboard = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'size' :size,\n",
    "                    'socket': socket,\n",
    "                    'chipset': chipset,\n",
    "                    'ramslots': ramslots,\n",
    "                    'memory_type': memory_type,\n",
    "                    'memory_capacity':memory_capacity,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                motherboards.append(motherboard)\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('motherboards.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Nombre', 'Size','Socket', 'Chipset', 'Ramslots','Memory_Type', 'Memory_Capacity','Producer', 'Precio'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for motherboard in motherboards:\n",
    "            writer.writerow([motherboard['nombre'],motherboard['size'],motherboard['socket'], motherboard['chipset'],motherboard['ramslots'],  \n",
    "                             motherboard['memory_type'], motherboard['memory_capacity'],motherboard['producer'], motherboard['precio']])\n",
    "            \n",
    "scrape_motherboards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866993f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89f731b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las ram \n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_rams():\n",
    "    url = \"https://www.pc-kombo.com/us/components/rams\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    rams = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "            size = i.find(\"span\", {'class': 'size'}).text[:-2]\n",
    "            types = i.find(\"span\", {'class': 'type'}).text\n",
    "            \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_ram = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_ram)\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                #obtener el clock\n",
    "                clock = soup.find('dt', text='Clock')\n",
    "                if clock is not None:\n",
    "                    clock = clock.find_next_sibling('dd').text\n",
    "                \n",
    "                #obtener los sticks\n",
    "                sticks = soup.find('dt', text='Sticks')\n",
    "                if sticks is not None:\n",
    "                    sticks = sticks.find_next_sibling('dd').text \n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                ram = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'size' :size,\n",
    "                    'type' : types,\n",
    "                    'clock': clock,\n",
    "                    'sticks': sticks,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                rams.append(ram)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('rams.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Size', 'Type', 'Clock', 'Sticks', 'Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for ram in rams:\n",
    "            writer.writerow([ram['nombre'],ram['size'],ram['type'],ram['clock'],ram['sticks']\n",
    "                            ,ram['producer'],ram['precio']])\n",
    "            \n",
    "scrape_rams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6f0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f740b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las HDD\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_hdds():\n",
    "    url = \"https://www.pc-kombo.com/us/components/hdds\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    hdds = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "            size = i.find(\"span\", {'class': 'size'}).text[:-2]\n",
    "            rpm = i.find(\"span\", {'class': 'rpm'}).text\n",
    "            \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_hdd = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_hdd)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                hdd = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'size' :size,\n",
    "                    'rpm' : rpm,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                hdds.append(hdd)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('hdds.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Size', 'Rpm', 'Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for hdd in hdds:\n",
    "            writer.writerow([hdd['nombre'],hdd['size'],hdd['rpm']\n",
    "                            ,hdd['producer'],hdd['precio']])\n",
    "            \n",
    "scrape_hdds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07302594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "819cae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las SSD\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_ssds():\n",
    "    url = \"https://www.pc-kombo.com/us/components/ssds\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    ssds = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "            size = i.find(\"span\", {'class': 'size'}).text[:-2]\n",
    "            \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_hdd = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_hdd)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                #obtener el form factor\n",
    "                form_factor = soup.find('dt', text='Form Factor')\n",
    "                if form_factor is not None:\n",
    "                    form_factor = form_factor.find_next_sibling('dd').text\n",
    "                \n",
    "                #obtener el protocolo\n",
    "                protocolo = soup.find('dt', text='Protocol')\n",
    "                if protocolo is not None:\n",
    "                    protocolo = protocolo.find_next_sibling('dd').text\n",
    "\n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                ssd = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'size' :size,\n",
    "                    'form_factor':form_factor,\n",
    "                    'protocolo':protocolo,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                ssds.append(ssd)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('ssds.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Size', 'Form Factor','Protocol', 'Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for ssd in ssds:\n",
    "            writer.writerow([ssd['nombre'],ssd['size'],ssd['form_factor'],ssd['protocolo']\n",
    "                            ,ssd['producer'],ssd['precio']])\n",
    "            \n",
    "scrape_ssds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15e2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bde74690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las Power Supplies\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_ssds():\n",
    "    url = \"https://www.pc-kombo.com/us/components/psus\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    pss = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "            size = i.find(\"span\", {'class': 'size'}).text[:-2]\n",
    "            watts = i.find(\"span\", {'class': 'watt'}).text[:-1]\n",
    "            \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_hdd = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_hdd)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                #obtener el efficiency\n",
    "                efficiency = soup.find('dt', text='Efficiency Rating')\n",
    "                if efficiency is not None:\n",
    "                    efficiency = efficiency.find_next_sibling('dd').text\n",
    "                \n",
    "                #obtener los 8 pin\n",
    "                eight_pin = soup.find('dt', text='PCI-E cables 8-pin')\n",
    "                if eight_pin is not None:\n",
    "                    eight_pin = eight_pin.find_next_sibling('dd').text\n",
    "                else:\n",
    "                    eight_pin = 0\n",
    "                \n",
    "                #obtener los 6 pin\n",
    "                six_pin = soup.find('dt', text='PCI-E cables 6-pin')\n",
    "                if six_pin is not None:\n",
    "                    six_pin = six_pin.find_next_sibling('dd').text\n",
    "                else:\n",
    "                    six_pin = 0\n",
    "\n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                ps = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'size' :size,\n",
    "                    'watts':watts,\n",
    "                    'efficiency':efficiency,\n",
    "                    'eight_pin': eight_pin,\n",
    "                    'six_pin': six_pin,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                pss.append(ps)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('powerSupplies.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Size', 'Watts','Efficiency Rating','8 Pin', '6 Pin', 'Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for ps in pss:\n",
    "            writer.writerow([ps['nombre'],ps['size'],ps['watts'],ps['efficiency'],\n",
    "                            ps['eight_pin'],ps['six_pin'],ps['producer'],ps['precio']])\n",
    "            \n",
    "scrape_ssds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee995312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d99379bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de los coolers\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_coolers():\n",
    "    url = \"https://www.pc-kombo.com/us/components/cpucoolers\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    coolers = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "      \n",
    "            \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_hdd = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_hdd)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                #obtener los sockets\n",
    "                sockets = soup.find('dt', text='Supported Sockets')\n",
    "                if sockets is not None:\n",
    "                    sockets = sockets.find_next_sibling('dd').text[11:]\n",
    "                \n",
    "                #obtener el height\n",
    "                height = soup.find('dt', text='Height')\n",
    "                if height is not None:\n",
    "                    height = height.find_next_sibling('dd').text[:-2]\n",
    "                \n",
    "                #obtener el tdp\n",
    "                tdp = soup.find('dt', text='TDP')\n",
    "                if tdp is not None:\n",
    "                    tdp = tdp.find_next_sibling('dd').text[:-2]\n",
    "\n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                cooler = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'sockets' :sockets,\n",
    "                    'height':height,\n",
    "                    'tdp':tdp,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                coolers.append(cooler)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('coolers.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Sockets', 'Height','TDP','Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for cooler in coolers:\n",
    "            writer.writerow([cooler['nombre'],cooler['sockets'],cooler['height'],cooler['tdp'],\n",
    "                         cooler['producer'],cooler['precio']])\n",
    "            \n",
    "scrape_coolers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbacbf32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0055ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporta los datos necesarios de las cases\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def make_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_cases():\n",
    "    url = \"https://www.pc-kombo.com/us/components/cases\"\n",
    "    soup = make_soup(url)\n",
    "\n",
    "    cases = []\n",
    "    for i in soup.find(\"ol\", {'id': 'hardware'}).find_all('li'):\n",
    "        if(i != 'None' and i != -1):\n",
    "            nombre = i.find(\"h5\", {'class': 'name'})\n",
    "      \n",
    "          \n",
    "            # buscar el precio en el HTML\n",
    "            precio_html = i.find(\"span\", {'class': 'price'})\n",
    "            if precio_html is not None:\n",
    "                # obtener el valor del precio y asignarlo a la variable precio\n",
    "                precio = precio_html.text.strip()[4:]\n",
    "                \n",
    "            if precio != '':\n",
    "                url_hdd = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "                response = requests.get(url_hdd)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                #obtener el width\n",
    "                width = soup.find('dt', text='Width')\n",
    "                if width is not None:\n",
    "                    width = width.find_next_sibling('dd').text[:-2]\n",
    "                    \n",
    "                #obtener el Depth\n",
    "                depth = soup.find('dt', text='Depth')\n",
    "                if depth is not None:\n",
    "                    depth = depth.find_next_sibling('dd').text[:-2]\n",
    "                \n",
    "                #obtener el height\n",
    "                height = soup.find('dt', text='Height')\n",
    "                if height is not None:\n",
    "                    height = height.find_next_sibling('dd').text[:-2]\n",
    "                \n",
    "                #obtener la mÃ¡xima length de gpu prmitida\n",
    "                max_gpu_length = soup.find('dt', text='Supported GPU length')\n",
    "                if max_gpu_length is not None:\n",
    "                    max_gpu_length = max_gpu_length.find_next_sibling('dd').text[:-2]\n",
    "\n",
    "                \n",
    "                # Obtener el productor\n",
    "                producer = soup.find(\"dd\",{'itemprop':'brand'} ).text\n",
    "                    \n",
    "            \n",
    "\n",
    "                case = {\n",
    "                    'nombre': nombre.get_text(strip=True),\n",
    "                    'width' :width,\n",
    "                    'depth':height,\n",
    "                    'height':height,\n",
    "                    'max_gpu_length': max_gpu_length,\n",
    "                    'producer': producer,\n",
    "                    'precio': precio\n",
    "                }\n",
    "                \n",
    "                cases.append(case)\n",
    "\n",
    "\n",
    "    # Escribir los datos en un archivo CSV\n",
    "    with open('cases.csv', mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        # Escribir la cabecera del archivo CSV\n",
    "        writer.writerow(['Name', 'Width mm', 'Depth mm','Height mm',' Maximum GPU Length','Producer', 'Price'])\n",
    "        # Escribir cada fila en el archivo CSV\n",
    "        for case in cases:\n",
    "            writer.writerow([case['nombre'],case['width'],case['depth'],case['height'],\n",
    "                         case['max_gpu_length'],case['producer'],case['precio']])\n",
    "            \n",
    "scrape_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d5d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url = \"https://www.pc-kombo.com/us/product/motherboard/4719072733667_MSI%20B550-A%20Pro\"\n",
    "soup = make_soup(url)\n",
    "\n",
    "price = soup.find(\"body\").find(\"section\", {'class': 'column col-12 col-md-12'}).find('span', {'itemprop':'price'}).text\n",
    "print(\"PRECIO:\", price)\n",
    "list_a = soup.find(\"body\").find('div', {'class': 'container grid-xl'}).find('div', {'class': 'columns'}).find_all('dd')\n",
    "list_b = soup.find(\"body\").find('div', {'class': 'container grid-xl'}).find('div', {'class': 'columns'}).find_all('dt')\n",
    "\n",
    "for i in range(len(list_a)):\n",
    "    print(list_b[i].text, \" \", list_a[i].text)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f509e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def scrapping_all_components(code):\n",
    "\n",
    "    url = \"https://www.pc-kombo.com/us/components/\" + code\n",
    "    soup = make_soup(url)\n",
    "    columns_scrapping = ['Producer', 'Length', 'Slots', '8-pin connectors', '6-pin connectors', 'Vram',\n",
    "                         'Boost Clock', 'Memory Clock']\n",
    "    lista = [m for m in soup.find(\"body\").find(\"ol\", {'id': 'hardware'}).find_all('li')]\n",
    "    final = []\n",
    "    ibi = 0\n",
    "    \n",
    "    for i in lista:\n",
    "\n",
    "        i = (i.find('a', href=True)['href']).split(\" \")[0]\n",
    "\n",
    "        if(i != 'None' and i != -1):\n",
    "            ibi+=1\n",
    "            print(i)\n",
    "\n",
    "            url_2 = i\n",
    "            soup_2 = make_soup(url_2)\n",
    "            \n",
    "            name = soup_2.find(\"body\").find(\"h1\", {\"itemprop\":\"name\"}).text\n",
    "            \n",
    "            rating = str(soup_2.find(\"body\").find('p', {'itemprop' : 'aggregateRating'}).text).replace(\" \", \"\")\n",
    "            rating = rating.replace('\\n', '').replace('\\r', '')\n",
    "            rating = rating[-2:-9:-1]\n",
    "            price = soup_2.find(\"body\").find(\"section\", {'class': 'column col-12 col-md-12'}).find('span', {'itemprop':'price'})\n",
    "            list_a = soup_2.find(\"body\").find('div', {'class': 'container grid-xl'}).find('div', {'class': 'columns'}).find_all('dd')\n",
    "            list_b = soup_2.find(\"body\").find('div', {'class': 'container grid-xl'}).find('div', {'class': 'columns'}).find_all('dt')\n",
    "            \n",
    "            #ELEMNTOS PARA LA TABLA: NAME, PRODUCER, MODEL, LENGTH, SLOTS, 8_PIN, 6_PIN,\n",
    "            # VRAM, TDP, BOOST_CLOCK, MEMORY_CLOCK, PERFORMANCE_LEVEL, PRICE\n",
    "            if price == None:\n",
    "                print(name + \",\" + str(price) + \",\" + rating[::-1] + \",\",  end = \"\")\n",
    "            else:\n",
    "                print(name + \",\" + str(price.txt) + \",\" + rating[::-1] + \",\", end = \"\")\n",
    "\n",
    "            for i in range(len(list_a)):\n",
    "                \n",
    "                if list_b[i].text in columns_scrapping:\n",
    "                    print(list_a[i].text, end = \"\")\n",
    "                    print(\",\", end=\"\")\n",
    "                   \n",
    "            print(\"\")\n",
    "\n",
    "    print(ibi)\n",
    "    \n",
    "scrapping_all_components(\"gpus\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcb9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELEMNTOS PARA LA TABLA: NAME, PRODUCER, MODEL, LENGTH, SLOTS, 8_PIN, 6_PIN, VRAM, TDP, BOOST_CLOCK, MEMORY_CLOCK, PERFORMANCE_LEVEL, PRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056436c4",
   "metadata": {},
   "source": [
    "# ORDENADOR PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import cpuinfo\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def unidades(bytes, suffix=\"B\"):\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "\n",
    "print(f\"NOMBRE DEL EQUIPO: {platform.node()}\")\n",
    "print(f\"SISTEMA OPERATIVO: {platform.system()} con VERSIÃN {platform.version()}\")\n",
    "print(f\"PROCESADOR: {cpuinfo.get_cpu_info()['brand_raw']} CON {psutil.cpu_count(logical=False)} NÃCLEOS\")\n",
    "print(f\"GRAFICA: {GPUtil.getGPUs()[0].name}\")\n",
    "print(f\"MEMORIA RAM: {unidades(psutil.virtual_memory().total)}\")\n",
    "partition_usage = psutil.disk_usage(psutil.disk_partitions()[0].mountpoint)\n",
    "print(f\"INFORMACION DEL DISCO: \")\n",
    "print(f\"  TAMAÃO TOTAL: {unidades(partition_usage.total)}\")\n",
    "print(f\"  USADO: {unidades(partition_usage.used)}\")\n",
    "print(f\"  DISPONIBLE: {unidades(partition_usage.free)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b35c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
